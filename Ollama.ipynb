{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ElXjyv2U6n2f"},"outputs":[],"source":["#installing ollama on host\n","!curl https://ollama.ai/install.sh | sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2cjE1fG60zY"},"outputs":[],"source":["# install dependencies for python script\n","!pip install pyngrok\n","\n","import os\n","import asyncio\n","from google.colab import userdata\n","\n","# set LD_LIBRARY_PATH so system NVIDIA library becomes preferred\n","# over the built-in library. This is particularly important for\n","# Google Colab which installs older drivers\n","os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n","\n","async def run_process(cmd):\n","    '''\n","    run is a helper function to run subcommands asynchronously.\n","    '''\n","    # create subprocess and collect the output\n","    print('>>> starting', *cmd)\n","    p = await asyncio.subprocess.create_subprocess_exec(\n","        *cmd,\n","        stdout=asyncio.subprocess.PIPE,\n","        stderr=asyncio.subprocess.PIPE,\n","    )\n","\n","    # print lines utf-8 encoded\n","    async def pipe(lines):\n","      async for line in lines:\n","        print(line.strip().decode('utf-8'))\n","\n","    await asyncio.gather(\n","        pipe(p.stdout),\n","        pipe(p.stderr),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upI2quMbmccj"},"outputs":[],"source":["token = userdata.get(\"NGROKTOKEN\")\n","from pyngrok import ngrok\n","!ngrok config add-authtoken {token}\n","# !cat /root/.config/ngrok/ngrok.yml"]},{"cell_type":"code","source":["# install streamlit for app ui along with additional required packages\n","!pip install -q streamlit langchain langchain_community BeautifulSoup4 tiktoken chromadb"],"metadata":{"id":"X4nanZqhp8B9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run ollama sub process, then run streamlit app and expose it via ngrok\n","# streamlit app should be upploaded to files under /content\n","await asyncio.gather(\n","    run_process(['ollama', 'serve']),\n","    run_process(['ollama', 'pull', 'nomic-embed-text']),\n","    run_process(['ollama', 'run', 'mistral']),\n","    run_process(['streamlit', 'run', 'app.py']),\n","    run_process(['ngrok', 'http', '--log', 'stderr', '8501']),\n",")\n"],"metadata":{"id":"qt-yVnfsqJ70"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLmxe9sZmiEh"},"outputs":[],"source":["# run ollama sub process and expose it from localhost:11434 to public via http\n","# await asyncio.gather(\n","#     run_process(['ollama', 'serve']),\n","#     run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header=\"localhost:11434\", '--request-header=\"localhost:11434\"']),\n","# )\n","\n","# !ollama serve\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hYXK9mPloO6"},"outputs":[],"source":["# This line will shut down the ngrok agent\n","ngrok.kill()"]},{"cell_type":"markdown","metadata":{"id":"OaynA5WPbkV0"},"source":["# https://github.com/ollama/ollama/blob/main/examples/jupyter-notebook/ollama.ipynb\n","\n","# https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}